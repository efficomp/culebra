# This file is part of culebra.
#
# Culebra is free software: you can redistribute it and/or modify it under the
# terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# Culebra is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
# A PARTICULAR PURPOSE. See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# Culebra. If not, see <http://www.gnu.org/licenses/>.
#
# This work was supported by project PGC2018-098813-B-C31 (Spanish "Ministerio
# de Ciencia, Innovación y Universidades"), and by the European Regional
# Development Fund (ERDF).

"""This module provides some classes to make the experimentation easier.

  * :py:class:`~experiment.ConfigManager` class: Build an
    :py:class:`~experiment.Experiment` or a :py:class:`~experiment.Batch` of
    experiment from a TOML configuration file.
  * :py:class:`~experiment.Base` class: Base class for
    :py:class:`~experiment.Experiment` and :py:class:`~experiment.Batch`
  * :py:class:`~experiment.Experiment` class: Run an experiment
  * :py:class:`~experiment.Batch` class: Run a batch of experiments
"""

from os import path
from pydoc import locate
import shutil
import copy
import os
import toml
import numpy as np
import pandas as pd
from culebra.base import Base as CulebraBase
from culebra.base import Dataset, Metrics


__author__ = 'Jesús González'
__copyright__ = 'Copyright 2021, EFFICOMP'
__license__ = 'GNU GPL-3.0-or-later'
__version__ = '0.1.1'
__maintainer__ = 'Jesús González'
__email__ = 'jesusgonzalez@ugr.es'
__status__ = 'Development'


class Labels:
    """Labels for the results dataframes."""

    individual = 'Individual'
    """Label for the individual column in the dataframes"""

    feature = 'Feature'
    """Label for the feature column in the dataframes"""

    phase = 'Phase'
    """Label for the phase column in the dataframes"""

    training = 'Training'
    """Training phase label"""

    test = 'Test'
    """Test phase label"""

    fitness = 'Fitness'
    """Label for the fitness column in the dataframes"""

    relevance = 'Relevance'
    """Label for the relevance column in the dataframes"""

    rank = 'Rank'
    """Label for the rank column in the dataframes"""

    max = 'Max'
    """Label for the max column in the dataframes"""

    min = 'Min'
    """Label for the min column in the dataframes"""

    avg = 'Avg'
    """Label for the avg column in the dataframes"""

    std = 'Std'
    """Label for the std column in the dataframes"""

    best = 'Best'
    """Label for the best column in the dataframes"""

    stat = 'Stat'
    """Label for the stat column in the dataframes"""

    metric = 'Metric'
    """Label for the metric column in the dataframes"""

    runtime = "Runtime"
    """Label for the runtime column in dataframes."""

    experiment = "Exp"
    """Label for the experiment column in dataframes."""

    batch = "Batch"
    """Label for the batch column in dataframes."""


class Files:
    """Files generated by an experiment."""

    script = "run.py"
    """Name for the script to run an experiment."""

    backup = "backup.gz"
    """Default name for the file to backup the experiment."""

    results = "results.xlsx"
    """Name for the file to store the results."""


DEFAULT_TEST_STATS = {Labels.avg: np.mean,
                      Labels.std: np.std,
                      Labels.min: np.min,
                      Labels.max: np.max}
"""Default statistics calculated for tested solutions."""

DEFAULT_FEATURE_METRICS = {Labels.relevance: Metrics.relevance,
                           Labels.rank: Metrics.rank}
"""Default metrics calculated for the features in the set of solutions."""

DEFAULT_BATCH_STATS = {Labels.avg: pd.DataFrame.mean,
                       Labels.std: pd.DataFrame.std,
                       Labels.min: pd.DataFrame.min,
                       Labels.max: pd.DataFrame.max}
"""Default statistics calculated for the results gathered from all the
experiments."""

DEFAULT_N_EXPERIMENTS = 1
"""Default number of experiments in the batch."""


class Base(CulebraBase):
    """Base class for experiments."""

    _results = {}
    """Results provided by the experiment."""

    _script_code = ""
    """Parameterized script to run this experiment."""

    @property
    def results(self):
        """Get all the results provided by this batch of experiments.

        :type: :py:class:`dict`
        """
        return self._results

    def run(self):
        """Run the experiment.

        This method must be overriden by subclasses to return a correct
        value.
        """
        raise NotImplementedError("The run method has not been implemented in "
                                  f"the {self.__class__.__name__} class")

    def save(self, file=None):
        """Save this experiment.

        :param file: File to store the experiment. Defaults to
            :py:attr:`~experiment.Files.backup_file`
        :type file: A path to a file, optional.
        """
        file = Files.backup if file is None else file
        pd.io.pickle.to_pickle(self, file)

    @staticmethod
    def load(file=None):
        """Load an experiment.

        :param file: File to load the experiment from. Defaults to
            :py:attr:`~experiment.Files.backup_file`
        :type file: A path to a file, optional.
        """
        file = Files.backup if file is None else file
        return pd.io.pickle.read_pickle(file)

    @classmethod
    def generate_script(cls, config, file=None):
        """Generate a script to run the experiment.

        :param config: Path to the configuration file
        :type config: :py:class:`str`
        :param file: File to store the script. Defaults to
            :py:attr:`~experiment.Files.script_file`
        :type file: A path to a file, optional.
        """
        file = Files.script if file is None else file

        # Create the script
        with open(file, 'w') as script:
            script.write(cls._script_code.format_map(
                {"config_file": config, "res": "{res}"}))

        # Make the script executable
        os.chmod(file, 0o777)

    def results_to_excel(self, file=None):
        """Save the results to a Excel file.

        :param file: File to store the results. Defaults to
            :py:attr:`~experiment.Files.results_file`
        :type file: A path to a file, optional.
        """
        file = Files.results if file is None else file

        with pd.ExcelWriter(file) as writer: \
                # pylint: disable=abstract-class-instantiated
            for name, data in self.results.items():
                data.to_excel(writer, sheet_name=name)

            writer.save()

    def _reset(self):
        """Reset the results."""
        self._results = {}


class Experiment(Base):
    """Run a wrapper method from the parameters in a config file."""

    _default_feature_metrics = DEFAULT_FEATURE_METRICS
    """Metrics calculated for the features in the set of solutions."""

    _default_stats = DEFAULT_TEST_STATS
    """Statistics calculated for the tested solutions."""

    _script_code = """#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from culebra.experiment import ConfigManager

# Parameter file
CONFIG = "{config_file}"

# Get the configuration parameters
config_manager = ConfigManager(CONFIG)

# Create the experiment
experiment = config_manager.build_experiment()

# Run the experiment
experiment.run()

# Save the experiment
experiment.save()

# Save the results to Excel
experiment.results_to_excel()

# Print the results
for res, val in experiment.results.items():
    print(f"\\n\\n{res}:")
    print(val)
"""
    """Parameterized script to run this experiment."""

    def __init__(self, wrapper, tr_data, tr_fitness, tst_data=None,
                 tst_fitness=None):
        """Set an experiment to run a wrapper method.

        :param wrapper: The wrapper method
        :type wrapper: Any subclass of :py:class:`~base.Wrapper`.
        :param tr_data: Training data
        :type tr_data: :py:class:`~base.Dataset`
        :param tr_fitness: The fitness used for training
        :type tr_fitness: Any subclass of :py:class:`~base.Fitness`
        :param tst_data: Test data. If `None`, *tr_data* will be used. Defaults
            to `None`
        :type tst_data: :py:class:`~base.Dataset`, optional
        :param tst_fitness: The fitness used to test. If `None`, *tr_fitness*
            will be used. Defaults to `None`.
        :type tst_fitness: Any subclass of :py:class:`~base.Fitness`,
            optional
        """
        self._wrapper = wrapper

        self._tr_data = tr_data
        self._tst_data = tst_data if tst_data else tr_data

        self._tr_fitness = tr_fitness
        self._tst_fitness = tst_fitness if tst_fitness else tr_fitness

        # Reset results
        self._reset()

    def run(self):
        """Run the wrapper method."""
        # Forget previous results
        self._reset()

        hof, runtime = self.__train()

        # Insert the runtime ina dataframe
        self.__add_execution_metrix(Labels.runtime, runtime)

        # Get the features stats
        self.__do_feature_metrics(hof)

        # Test the best solutions found
        hof = self.__test(hof)

        # Get the best solutions found
        self.__do_best_solutions(hof)

    def __add_execution_metrix(self, metric, value):
        """Add an execution metric.

        :param metric: Name of the metric
        :type metric: :py:class:`str`
        :param value: Value of the metric
        :type value: :py:class:`object`
        """
        # Name of the result
        result_name = 'execution_metrics'

        # Create the DataFrame if it doesn't exist
        if result_name not in self._results:
            self._results[result_name] = pd.DataFrame()
            self._results[result_name].columns.set_names(
                [Labels.metric], inplace=True)

        self._results[result_name][metric] = [value]

    def __do_best_solutions(self, hof):
        """Generate a DataFrame with the best solutions found.

        :param hof: The best individuals found
        :type hof: :py:class:`~deap.tools.HallOfFame` of individuals
        """
        # Find the best wvalue for each objectuve
        best_wvalues = None
        for ind in hof:
            # Init
            if best_wvalues is None:
                fitness_names = ind.fitness.names
                best_wvalues = list(ind.fitness.wvalues)
                num_obj = ind.fitness.n_obj
                sim_thresholds = ind.fitness.thresholds

            # Update the best wvalues
            for obj in range(num_obj):
                if ind.fitness.wvalues[obj] > best_wvalues[obj]:
                    best_wvalues[obj] = ind.fitness.wvalues[obj]

        # Columns for the dataframe
        best_fitness_names = []
        best_values = []
        best_inds = []

        # Find the individuals having wvalues similar to the best ones
        for ind in hof:
            for obj, threshold in zip(range(num_obj), sim_thresholds):
                if best_wvalues[obj] - ind.fitness.wvalues[obj] <= threshold:
                    best_fitness_names += [fitness_names[obj]]
                    best_values += [ind.fitness.values[obj]]
                    best_inds += [ind]

        # Name of the result
        result_name = 'best_solutions'

        # Create the dataframe
        self._results[result_name] = pd.DataFrame()

        # Insert the data
        self._results[result_name][Labels.fitness] = best_fitness_names
        self._results[result_name][Labels.test] = best_values
        self._results[result_name][Labels.individual] = best_inds

        self._results[result_name].set_index([Labels.fitness], inplace=True)
        self._results[result_name].sort_index(inplace=True)
        self._results[result_name].columns.set_names(
            [Labels.best], inplace=True)

    def __do_training_solutions(self, hof, *fitness_names):
        """Add the training fitness values to the solutions found.

        :param hof: Individuals to be appended
        :type hof: :py:class:`~deap.tools.HallOfFame` of individuals
        :param fitness_names: Fitness names
        :type fitness_names: :py:class:`list`
        """
        # Name of the result
        result_name = 'solutions'

        # Create the solutions dataframe
        self._results[result_name] = pd.DataFrame(
                columns=[Labels.individual, Labels.fitness,
                         Labels.training])

        # For each solution found
        for ind in hof:
            # For each fitness objective
            for name, val in zip(fitness_names, ind.fitness.values):
                data = {Labels.individual: ind,
                        Labels.fitness: name,
                        Labels.training: val}
                self._results[result_name] = (
                    self._results[result_name].append(data, ignore_index=True))

    def __do_test_solutions(self, hof):
        """Add the test fitness values to the solutions found.

        :param hof: Individuals to be appended
        :type hof: :py:class:`~deap.tools.HallOfFame` of individuals
        """
        values = []
        # For each solution found
        for ind in hof:
            # For each fitness objective
            for val in ind.fitness.values:
                values += [val]

        # Name of the result
        result_name = 'solutions'

        # Add the test fitness values
        self._results[result_name][Labels.test] = values

        self._results[result_name].set_index(
            [Labels.individual, Labels.fitness], inplace=True)
        self._results[result_name].sort_index(inplace=True)

        self._results[result_name].columns.set_names(
            [Labels.phase], inplace=True)

    def __do_feature_metrics(self, hof):
        """Perform stats about features frequency in the set of solutions.

        :param hof: The best individuals found
        :type hof: :py:class:`~deap.tools.HallOfFame` of individuals
        """
        # Name of the result
        result_name = 'feature_metrics'

        # Create the dataframe
        self._results[result_name] = pd.DataFrame()

        # Perform the stats
        for name, func in self._default_feature_metrics.items():
            self._results[result_name][name] = func(hof)

        self._results[result_name].index.names = [Labels.feature]
        self._results[result_name].columns.set_names(
            [Labels.metric], inplace=True)

    def __do_training_stats(self, logbook):
        """Perform the training stats from a logbook.

        :param logbook: The logbook
        :type logbook: :py:class:`~deap.tools.Logbook`
        """
        # Number of generations in the logbook
        n_gens = len(logbook)

        # Number of objectives
        n_obj = self._tr_fitness.n_obj

        # Name of the result
        result_name = 'training_stats'

        # Create the dataframe
        self._results[result_name] = pd.DataFrame()

        # Add the generational stats
        index = []
        for stat in self._wrapper.stats_names:
            self._results[result_name][stat.capitalize()] = \
                logbook.select(stat)*n_obj
            index += [stat.capitalize()]

        # Index for each objective stats
        fitness_index = []
        for i in range(n_obj):
            fitness_index += [self._tr_fitness.names[i]]*n_gens
        self._results[result_name][Labels.fitness] = fitness_index

        # For each objective stat
        for stat in self._wrapper.objective_stats_names:
            # Select the data of this stat for all the objectives
            data = logbook.select(stat)
            stat_data = np.zeros(n_gens*n_obj)
            for i in range(n_gens):
                for j in range(len(data[i])):
                    stat_data[j*n_gens + i] = data[i][j]

            self._results[result_name][stat.capitalize()] = stat_data

        self._results[result_name].set_index(index + [Labels.fitness],
                                             inplace=True)
        self._results[result_name].sort_index(inplace=True)
        self._results[result_name].columns.set_names(
            [Labels.stat], inplace=True)

    def __do_test_stats(self, hof):
        """Perform some stats on the tested solutions.

        :param hof: The best individuals found (evaluated with test data)
        :type hof: :py:class:`~deap.tools.HallOfFame` of individuals
        """
        # Number of objectives
        n_obj = self._tst_fitness.n_obj

        # Number of individuals in the hof
        n_ind = len(hof)

        # Array to store all the fitnesses
        fitness = np.zeros([n_obj, n_ind])

        # Check all the solutions
        for i, ind in zip(range(n_ind), hof):
            fitness[:, i] = ind.fitness.values

        # Name of the result
        result_name = 'test_stats'

        # Create the dataframe
        self._results[result_name] = pd.DataFrame()

        # Perform the stats
        self._results[result_name][Labels.fitness] = self._tst_fitness.names
        for name, func in self._default_stats.items():
            self._results[result_name][name] = func(fitness, axis=1)

        self._results[result_name].set_index([Labels.fitness], inplace=True)
        self._results[result_name].sort_index(inplace=True)
        self._results[result_name].columns.set_names([
                Labels.stat], inplace=True)

    def __train(self):
        """Perform the training step.

        Train the wrapper and append the solutions to the best solutions
        dataframe.

        :return: The best individuals found
        :rtype: :py:class:`~deap.tools.HallOfFame` of individuals
        :return: The runtime of the algorithm
        :rtype: :py:class:`float`
        """
        # Search the best solutions
        hof, logbook, runtime = self._wrapper.train(self._tr_data,
                                                    self._tr_fitness)

        # Add the training fitness to the best solutions dataframe
        self.__do_training_solutions(hof, *self._tr_fitness.names)

        # Get the training stats from the logbook
        self.__do_training_stats(logbook)

        # Return the best solutions found
        return hof, runtime

    def __test(self, hof):
        """Perform the test step.

        Test the solutions found by the wrapper append their fitness to the
        best solutions dataframe.

        :param hof: The best individuals found
        :type hof: :py:class:`~deap.tools.HallOfFame` of individuals
        :return: The best solutions found, updated with the test fitness
        :rtype: :py:class:`~deap.tools.HallOfFame` of individuals
        """
        # Test the best solutions found
        self._wrapper.test(hof, self._tst_data, self._tst_fitness)

        # Add the test fitness to the best solutions dataframe
        self.__do_test_solutions(hof)

        # Perform the test stats
        self.__do_test_stats(hof)

        # Update the hof
        hof.update(hof)

        return hof

    def __reduce__(self):
        """Reduce the experiment.

        :return: The reduction
        :rtype: :py:class:`tuple`
        """
        return (self.__class__,
                (self._wrapper, self._tr_data, self._tr_fitness),
                self.__dict__)

    def __copy__(self):
        """Shallow copy the experiment."""
        cls = self.__class__
        result = cls(self._wrapper, self._tr_data, self._tr_fitness)
        result.__dict__.update(self.__dict__)
        return result

    def __deepcopy__(self, memo):
        """Deepcopy the experiment.

        :param memo: Individual attributes
        :type memo: :py:class:`dict`
        :return: A deep copy of the individual
        :rtype: :py:class:`~base.Individual`
        """
        cls = self.__class__
        result = cls(self._wrapper, self._tr_data, self._tr_fitness)
        result.__dict__.update(copy.deepcopy(self.__dict__, memo))
        return result


class Batch(Base):
    """Generate a batch of experiments."""

    _default_stats = DEFAULT_BATCH_STATS
    """Statistics calculated for the results gathered from all the
    experiments."""

    _script_code = """#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from culebra.experiment import ConfigManager

# Parameter file
CONFIG = "{config_file}"

# Get the configuration parameters
config_manager = ConfigManager(CONFIG)

# Create the batch
batch = config_manager.build_batch()

# Run the experiments in the batch
batch.run()

# Save the batch
batch.save()

# Save the results to Excel
batch.results_to_excel()

# Print the results
for res, val in batch.results.items():
    print(f"\\n\\n{res}:")
    print(val)
"""
    """Script to run all the experiments in a batch."""

    def __init__(self, config_file, n_experiments=DEFAULT_N_EXPERIMENTS,
                 folder_path=None):
        """Generate a batch of experiments.

        :param config_file: Path to the configuration file
        :type config_file: :py:class:`str`
        :param n_experiments: Number of experiments in the batch, defaults to
            :py:attr:`~experiment.DEFAULT_N_EXPERIMENTS`
        :type n_experiments: :py:class:`int`, optional
        :param folder_path: Path for the batch folder. If `None` the current
            folder is assumed. Defaults to `None`
        :type folder_path: :py:class:`str`, optional
        """
        # Absolute path to current folder
        return_path = os.getcwd()

        # Absolute path to the batch folder
        self._path = os.path.abspath(folder_path) \
            if folder_path else return_path

        # Absolute path to the config file
        self._config_file = os.path.abspath(config_file)

        # Number of experiments
        self._n_experiments = n_experiments

        # Create the batch folder
        try:
            os.makedirs(self.path)
        except FileExistsError:
            # The directory already exists
            pass

        # Copy the config file in the batch folder
        try:
            shutil.copy(self.config_file, self.path)
        except shutil.SameFileError:
            # The file is already there
            pass

        # Change to batch folder
        os.chdir(self.path)

        # For all the experiments to be generated ...
        for exp in self.exp_folders:
            try:
                # Create the expetiment folder
                os.makedirs(exp)
            except FileExistsError:
                # The directory already exists
                pass

            # Change to the experiment folder
            os.chdir(exp)

            # Generate the script to run the experiment
            Experiment.generate_script(os.path.relpath(self.config_file))

            # Return to the batch folder
            os.chdir("..")

        # Generate the script to run the batch
        self.generate_script(os.path.relpath(self.config_file))

        # Return to the current folder
        os.chdir(return_path)

        # Reset results
        self._reset()

    def _reset(self):
        """Reset the results."""
        super()._reset()
        self._results_indices = {}

    @property
    def config_file(self):
        """Get the path to the configuration file.

        :type: :py:class:`str`
        """
        return self._config_file

    @property
    def path(self):
        """Get the path to the batch folder.

        :type: :py:class:`str`
        """
        return self._path

    @property
    def n_experiments(self):
        """Get the number of experiments in the batch.

        :type: :py:class:`int`
        """
        return self._n_experiments

    @property
    def exp_folders(self):
        """Get the experiment folders.

        :type: :py:class:`tuple` of :py:class:`str`
        """
        # Suffix length
        suffix_len = len((self.n_experiments-1).__str__())

        # Generator for the experiment names
        return tuple(
            Labels.experiment.lower() +
            f"{i:0{suffix_len}d}" for i in range(0, self.n_experiments))

    def run(self):
        """Run a batch of experiments."""
        # Absolute path to current folder
        return_path = os.getcwd()

        # Change to the batch path
        os.chdir(self.path)

        # For all the experiments ...
        for exp in self.exp_folders:
            # Enter the experiment folder
            os.chdir(exp)

            # Run the experiment script
            with open(Files.script, "rb") as script_file:
                code = compile(script_file.read(), Files.script, "exec")
            exec(code, {})

            # Return th the batch folder
            os.chdir("..")

        # Return to the current folder
        os.chdir(return_path)

        # Gather the results
        self.__gather_results()

    def __append_data(self, result_name, exp_name, exp_data):
        """Append data from an experiment to a result dataframe.

        :param result_name: Name of the result
        :type result_name: :py:class:`str`
        :param exp_name: Name of the experiment
        :type exp_name: :py:class:`str`
        :param exp_data: Data of the result
        :type exp_data: :py:class:`~pandas.Series` or
            :py:class:`~pandas.DataFrame`
        """
        # Column names of exp_data
        column_names = []

        # Create the dataframe if hasn't been created yet
        if result_name not in self._results:
            self._results[result_name] = pd.DataFrame()

            # Create the df index
            index = [Labels.experiment]

            if isinstance(exp_data, pd.DataFrame):
                if exp_data.index.names[0] is not None:
                    index += exp_data.index.names

            self._results_indices[result_name] = index

        # Complete the list of columns
        if exp_data.index.names[0] is not None:
            column_names += exp_data.index.names
        column_names += list(exp_data.columns)

        dataframe = pd.DataFrame()
        dataframe[Labels.experiment] = [exp_name]*len(exp_data.index)

        # Append the data
        if isinstance(exp_data, pd.DataFrame):
            exp_data.reset_index(inplace=True)
            dataframe[column_names] = exp_data[column_names]
        elif isinstance(exp_data, pd.Series):
            dataframe[exp_data.name] = exp_data
        else:
            raise TypeError("Only supported pandas Series and DataFrames")

        self._results[result_name] = (
            self._results[result_name].append(dataframe))

        self._results[result_name].columns.set_names(
            exp_data.columns.names, inplace=True)

    def __gather_results(self):
        """Gather the results of a batch of experiments."""
        # Absolute path to current folder
        return_path = os.getcwd()

        # Change to the batch path
        os.chdir(self.path)

        # For all the experiments ...
        for exp_folder in self.exp_folders:
            # Enter the experiment folder
            os.chdir(exp_folder)

            # Load the experiment
            exp = Experiment.load()

            # Append the experiment results
            for result, data in exp.results.items():
                self.__append_data(result, exp_folder, data)

            # Return th the batch folder
            os.chdir("..")

        # Sort the results dataframes
        for result_name in exp.results.keys():
            self._results[result_name].set_index(
                self._results_indices[result_name], inplace=True)
            self._results[result_name].sort_index(inplace=True)

        # Return to the current folder
        os.chdir(return_path)

        # Perform some stats
        self.__do_feature_metrics_stats()
        self.__do_best_solutions_stats()
        self.__do_execution_metrics_stats()

    def __do_execution_metrics_stats(self):
        """Perform some stats on the execution metrics."""
        # Name of the result
        result_name = 'execution_metrics_stats'
        input_data = 'execution_metrics'

        # Create the dataframe
        self._results[result_name] = pd.DataFrame(
            columns=list(self._default_stats))

        # For all the metrics
        for metric in self._results[input_data].columns:
            # New row for the dataframe
            stats = {Labels.metric: metric}

            # Apply the stats
            for name, func in self._default_stats.items():
                stats[name] = func(self._results[input_data][metric])

            # Append the new row
            self._results[result_name] = (
                    self._results[result_name].append(
                            stats, ignore_index=True))

        self._results[result_name].set_index(Labels.metric, inplace=True)
        self._results[result_name].sort_index(inplace=True)
        self._results[result_name].columns.set_names(
            [Labels.stat], inplace=True)

    def __do_best_solutions_stats(self):
        """Perform stats on the best solutions found for each objective."""
        # Name of the result
        result_name = 'best_solutions_stats'
        input_data = 'best_solutions'

        # Create the batch best solutions stats dataframe
        self._results[result_name] = pd.DataFrame(
                columns=list(self._default_stats))

        # Get the name of the different objectives
        fitness_index = self._results[input_data].index.levels[1]

        # Get the best value for all the objectives
        best_fitness = self._results[input_data][Labels.test]

        # For all the objectives
        for obj in fitness_index:

            # Best fitness values for obj
            best_for_obj = best_fitness[:, obj]

            # New row for the dataframe
            stats = {}

            # Apply the stats
            for name, func in self._default_stats.items():
                stats[name] = func(best_for_obj)

            # Append the new row
            self._results[result_name] = (
                self._results[result_name].append(stats, ignore_index=True))

        self._results[result_name].set_index(fitness_index, inplace=True)
        self._results[result_name].sort_index(inplace=True)
        self._results[result_name].columns.set_names(
            [Labels.stat], inplace=True)

    def __do_feature_metrics_stats(self):
        """Pertorm stats on the feature metrics of all the experiments."""
        # Name of the result
        result_name = 'feature_metrics_stats'
        input_data = 'feature_metrics'

        # Create the batch best solutions stats dataframe
        self._results[result_name] = pd.DataFrame(
                columns=list(self._default_stats))

        # Get the features
        features_index = self._results[input_data].index.levels[1]

        # For all the metrics
        for metric in self._results[input_data].columns:
            # Get the values of this metric
            metric_values = self._results[input_data][metric]

            # For all the features
            for feature in features_index:

                # Values for each feature
                best_for_obj = metric_values[:, feature]

                # New row for the dataframe
                stats = {Labels.metric: metric,
                         Labels.feature: feature}

                # Apply the stats
                for name, func in self._default_stats.items():
                    stats[name] = func(best_for_obj)

                # Append the new row
                self._results[result_name] = (
                    self._results[result_name].append(
                        stats, ignore_index=True))

        # Feature indices should be int
        self._results[result_name][Labels.feature] = \
            self._results[result_name][Labels.feature].astype(int)

        self._results[result_name].set_index(
            [Labels.metric, Labels.feature], inplace=True)
        self._results[result_name].sort_index(inplace=True)
        self._results[result_name].columns.set_names(
            [Labels.stat], inplace=True)

    def __reduce__(self):
        """Reduce the batch.

        :return: The reduction
        :rtype: :py:class:`tuple`
        """
        return (self.__class__,
                (self._config_file, ),
                self.__dict__)

    def __copy__(self):
        """Shallow copy the batch."""
        cls = self.__class__
        result = cls(self._config_file)
        result.__dict__.update(self.__dict__)
        return result

    def __deepcopy__(self, memo):
        """Deepcopy the batch.

        :param memo: Individual attributes
        :type memo: :py:class:`dict`
        :return: A deep copy of the individual
        :rtype: :py:class:`~base.Individual`
        """
        cls = self.__class__
        result = cls(self._config_file)
        result.__dict__.update(copy.deepcopy(self.__dict__, memo))
        return result


class ConfigSections:
    """Sections in the config file."""

    dataset = "dataset"
    """Section containing the dataset configuration."""

    wrapper = "wrapper"
    """Section containing the wrapper configuration."""

    batch = "batch"
    """Section containing the batch configuration."""

    species = "species"
    """Section containing the species configuration."""

    fitness = "fitness"
    """Section containing the fitness configuration."""

    training = "training"
    """Section containing the training configuration."""

    test = "test"
    """Section containing the test configuration."""

    classifier = "classifier"
    """Section containing the details of the classifier."""

    parameters = "parameters"
    """Section containing the parameters of any object or class."""


class ConfigKeys:
    """Keys in the config file."""

    test_prop = "test_prop"
    """Proportion of data used to test."""

    file = "file"
    """Inputs file name."""

    output_file = "output_file"
    """Output file name."""

    wrapper_cls = "wrapper_cls"
    """Key containing the wrapper class."""

    individual_cls = "individual_cls"
    """Key containing the individual class."""

    species_cls = "species_cls"
    """Key containing the species class."""

    fitness_cls = "fitness_cls"
    """Key containing the fitness class."""

    classifier_cls = "classifier_cls"
    """Key containing the classifier class."""


class ConfigManager(CulebraBase):
    """Manage the configuration file."""

    _toml_sep = "."
    """Section separator in TOML files."""

    _object_suffixes = ("_func", "_cls")
    """Key suffixes of values containing object names that should be replaced
    by the the object they name."""

    def __init__(self, config):
        """Create the manager from a TOML configuration file.

        :param config: Configuration file
        :type config: A path to a file
        """
        self._config_path = path.abspath(config)
        self._config = self.__replace_objects(toml.load(config))

    def load_dataset(self):
        """Parse the config parameters to load the training and test data.

        Once the configuration parameters are processed, this functions calls
        to :py:meth:`~base.dataset.Dataset.load_train_test` to load the
        data.

        :raises ValueError: If the structure of the config file is not valid.
        :return: A :py:class:`tuple` of :py:class:`~base.dataset.Dataset`
            containing the training and test datasets
        :rtype: :py:class:`tuple`
        """
        # Dataset parameters section name
        dataset_params_section = (
            ConfigSections.dataset + self._toml_sep +
            ConfigSections.parameters)

        # Get the dataset parameters
        dataset_params = self.__get_value(
            dataset_params_section, self._config, is_section=True)

        # Training dataset section name
        tr_dataset_section = (
            ConfigSections.dataset + self._toml_sep +
            ConfigSections.training)

        # Get the training dataset configuration
        tr_dataset_config = self.__get_value(
            tr_dataset_section, self._config, is_section=True, mandatory=True)

        # Get the training file names
        files = self.__get_dataset_files(
            tr_dataset_config, tr_dataset_section)

        # Check if the test data section should be considered
        # Get the dataset parameters
        test_prop_param = (
            dataset_params_section + self._toml_sep +
            ConfigKeys.test_prop)
        test_prop = self.__get_value(test_prop_param, self._config)

        # If test_prop is omitted, the test data must be loaded
        if test_prop is None:
            # Test dataset section name
            tst_dataset_section = (
                ConfigSections.dataset + self._toml_sep +
                ConfigSections.test)

            # Get the test dataset configuration
            tst_dataset_config = self.__get_value(
                tst_dataset_section, self._config, is_section=True)
            # Get the test file names
            files += self.__get_dataset_files(
                tst_dataset_config, tst_dataset_section)
        # Load and return the dataset
        return Dataset.load_train_test(*files, **dataset_params)

    def build_wrapper(self):
        """Return a wrapper method built from the configuration parameters.

        :raises ValueError: If the structure of the config file is not valid.
        :return: The wrapper object
        :rtype: A subclass of :py:class:`~base.wrapper.Wrapper`
        """
        # Get the wrapper configuration
        wrapper_config = self.__get_value(
            ConfigSections.wrapper, self._config, is_section=True,
            mandatory=True)
        # Get the wrapper class
        wrapper_cls = self.__get_value(
            ConfigKeys.wrapper_cls, wrapper_config,
            section_name=ConfigSections.wrapper, mandatory=True)

        # Get the individual class name
        individual_cls = self.__get_value(
            ConfigKeys.individual_cls, wrapper_config,
            section_name=ConfigSections.wrapper, mandatory=True)

        # Get the species config
        species_config = self.__get_value(
            ConfigSections.species, wrapper_config,
            section_name=ConfigSections.wrapper, is_section=True,
            mandatory=True)

        # Build the species
        species = self.__build_obj(
            ConfigKeys.species_cls, species_config,
            section_name=ConfigSections.wrapper + self._toml_sep +
            ConfigSections.species)

        # Get the wrapper parameters
        wrapper_params = self.__get_value(
            ConfigSections.parameters, wrapper_config,
            section_name=ConfigSections.wrapper, is_section=True)

        # Build the wrapper object
        return wrapper_cls(individual_cls, species, **wrapper_params)

    def build_fitness(self):
        """Return the training and test fitness objects.

        If the test fitness is omitted in the config file, the training
        fitness will also be returned as test fitness.

        :raises ValueError: If any mandatory value is missing
        :return: The training and test fitness
        :rtype: A :py:class:`tuple` composed by two instances of any subclass
            of :py:class:`~base.fitness.Fitness`
        """
        # Training fitness section name
        tr_fitness_section = (
            ConfigSections.fitness + self._toml_sep +
            ConfigSections.training)

        # Get the training fitness configuration
        tr_fitness_config = self.__get_value(
            tr_fitness_section, self._config, is_section=True, mandatory=True)

        # Get the training fitness
        tr_fitness = self.__build_fitness(
            tr_fitness_config, tr_fitness_section)

        # Test fitness section name
        tst_fitness_section = (
            ConfigSections.fitness + self._toml_sep + ConfigSections.test)

        # Get the test fitness
        tst_fitness_config = self.__get_value(
            tst_fitness_section, self._config, is_section=True)

        # If test fitness is omitted, the training fitness will be also used
        # to test
        if tst_fitness_config is None:
            tst_fitness = tr_fitness
        else:
            # Otherwise, get the test fitness
            tst_fitness = self.__build_fitness(
                tst_fitness_config, tst_fitness_section)

        return tr_fitness, tst_fitness

    def build_experiment(self):
        """Return an experiment to run the wrapper method.

        :raises ValueError: If any mandatory value is missing
        :return: The experiment
        :rtype: An :py:class:`~tools.experiment.Experiment` object
        """
        # Load the training and test data
        tr_data, tst_data = self.load_dataset()

        # Build the training and test fitness
        tr_fitness, tst_fitness = self.build_fitness()

        # Create the wrapper
        wrapper = self.build_wrapper()

        return Experiment(wrapper, tr_data, tr_fitness, tst_data, tst_fitness)

    def build_batch(self):
        """Return a batch of experiments to test the wrapper method.

        :raises ValueError: If any mandatory value is missing
        :return: The batch
        :rtype: A :py:class:`~tools.batch.Batch` object
        """
        # Get the wrapper configuration
        batch_config = self.__get_value(
            ConfigSections.batch, self._config, is_section=True)

        return Batch(self._config_path, **batch_config)

    def __replace_objects(self, config, key=None):
        """Replace all object names by the objects they name.

        Check all the keys in the config parameters and replaces the
        values of all the keys having a suffix in
        :py:attr:`~tools.config_manager.ConfigManager.object_suffixes`, which
        contain object names, by the objects they name.

        :param config: Configuration parameters
        :type config: Any valid TOML value
        :param key: A key, defaults to `None`
        :type key: :py:class:`str`, optional
        :raises ValueError: If any object can not be found.
        :return: The modiffied parameters
        :rtype: The same of *config*
        """
        if config is not None:
            if isinstance(config, dict):
                for name, value in config.items():
                    config[name] = self.__replace_objects(value, name)
            elif key is not None:
                for suffix in self._object_suffixes:
                    if key.endswith(suffix):
                        value = config
                        config = locate(value)
                        if config is None:
                            raise ValueError(f"Not valid value for {key}: "
                                             f"{value}")
                        break

        return config

    def __get_value(self, name, section_config, *, section_name=None,
                    is_section=False, mandatory=False):
        """Return a value from a section in a config file.

        :param name: Full name of the value.
        :type name: :py:class:`str`
        :param section_config: Section containing the value.
        :type section_config: :py:class:`dict`
        :param section_name: Name of the section containing the value, defaults
            to `None`
        :type section_name: :py:class:`str` or `None`
        :param is_section: `True` if the value is a section, defaults to
            `False`
        :type is_section: :py:class:`bool`
        :param mandatory: `True` if the value is mandatory, defaults to `False`
        :type mandatory: :py:class:`bool`
        :raises ValueError: If a mandatory value is missing
        :return: The value it it exists, or `None` otherwise
        :rtype: Any valid value in a TOML file
        """
        hierarchy = name.split(self._toml_sep)

        # The whole config file is used by default
        if section_config is None:
            value = self._config
        else:
            value = section_config
        for section in hierarchy:
            # Check the section exists
            if section in value.keys():
                value = value[section]
            else:
                if mandatory:
                    err_msg = "No "
                    if is_section:
                        err_msg += f"[{name}] section "
                    else:
                        err_msg += f"{name} key "

                    err_msg += "in "

                    if section_name is None:
                        err_msg += "the configuration file"
                    else:
                        err_msg += f"section [{section_name}]"

                    raise ValueError(err_msg)

                return {} if is_section else None

        return value

    def __get_dataset_files(self, dataset_config, section_name):
        """Return the dataset file names.

        :param dataset_config: Section where the file names are defined
        :type dataset_config: :py:class:`dict`
        :param section_name: Name of the section
        :type section_name: :py:class:`str`
        :raises ValueError: If the structure of *dataset_config* is not
            valid.
        :return: The file dataset file names
        :rtype: :py:class:`tuple`
        """
        # Get the inputs file name
        file_name = self.__get_value(
            ConfigKeys.file, dataset_config, section_name=section_name,
            mandatory=True)

        # Get the inputs file name
        output_file_name = self.__get_value(
            ConfigKeys.output_file, dataset_config,
            section_name=section_name)

        if output_file_name is not None:
            return (file_name, output_file_name)

        return (file_name,)

    def __build_obj(self, obj_key, obj_config, section_name):
        """Build an object.

        :param obj_key: Object key
        :type obj_key: :py:class:`str`
        :param obj_config: Section where the object is defined
        :type obj_config: :py:class:`dict`
        :param section_name: Name of the section
        :type section_name: :py:class:`str`
        :return: The object
        :rtype: Any object
        """
        # Get the object class
        obj_cls = self.__get_value(
            obj_key, obj_config,
            section_name=section_name, mandatory=True)

        # Get the object parameters
        obj_params = self.__get_value(
            ConfigSections.parameters, obj_config,
            section_name=section_name, is_section=True, mandatory=False)

        return obj_cls(**obj_params)

    def __build_fitness(self, fitness_config, section_name):
        """Build a fitness object.

        :param fitness_config: Section where the fitness is defined
        :type fitness_config: :py:class:`dict`
        :param section_name: Name of the section
        :type section_name: :py:class:`str`
        :return: The fitness object
        :rtype: Any subclass of :py:class:`~base.fitness.Fitness`
        """
        # Get the fitness class
        fitness_cls = self.__get_value(
            ConfigKeys.fitness_cls, fitness_config,
            section_name=section_name, mandatory=True)

        # Get the fitness parameters
        fitness_params = self.__get_value(
            ConfigSections.parameters, fitness_config,
            section_name=section_name, is_section=True)

        # Get the classifier configuration
        classifier_config = self.__get_value(
            ConfigSections.classifier, fitness_config,
            section_name=section_name, is_section=True, mandatory=True)

        # Build the classifier
        classifier = self.__build_obj(
            ConfigKeys.classifier_cls, classifier_config,
            section_name=section_name + self._toml_sep +
            ConfigSections.classifier)

        # Add the classifier to the fitness parameters
        fitness_params[ConfigSections.classifier] = classifier

        return fitness_cls(**fitness_params)

    def __reduce__(self):
        """Reduce the config manager.

        :return: The reduction
        :rtype: :py:class:`tuple`
        """
        return (self.__class__,
                (self._config, ),
                self.__dict__)

    def __copy__(self):
        """Shallow copy the config manager."""
        cls = self.__class__
        result = cls(self._config)
        result.__dict__.update(self.__dict__)
        return result

    def __deepcopy__(self, memo):
        """Deepcopy the config manager.

        :param memo: Individual attributes
        :type memo: :py:class:`dict`
        :return: A deep copy of the individual
        :rtype: :py:class:`~base.Individual`
        """
        cls = self.__class__
        result = cls(self._config)
        result.__dict__.update(copy.deepcopy(self.__dict__, memo))
        return result
